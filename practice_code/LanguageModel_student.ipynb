{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_rmWcSt5yVD"
      },
      "source": [
        "**Example of PennTreeBank Language Modeling**\n",
        "\n",
        "---\n",
        "\n",
        "make sure that you have these two directories (upload from ./tutorials-master)\n",
        "\n",
        "*   drive/My Drive/public/data/ptb\n",
        "*   drive/My Drive/public/results\n",
        "\n",
        "make sure that you have three dataset files in 'drive/My Drive/public/data/ptb/' directory\n",
        "\n",
        "*   ptb.train.txt\n",
        "*   ptb.train.txt.pkl\n",
        "*   ptb.valid.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyQaleE_5nYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "91f0b816-0570-458b-e758-b948204cdd18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5WpTM3o89aR"
      },
      "source": [
        "import pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN4q5hXC04t-"
      },
      "source": [
        "### Import the libraries\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import six; from six.moves import cPickle as pkl\n",
        "import numpy as np\n",
        "\n",
        "print(\"Importing libraries done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3ymLZ79RiU"
      },
      "source": [
        "define special tokens and util functinos\n",
        "\n",
        "\n",
        "*   ids2words : transform the tokenized sentence to real language sentence\n",
        "*   timeSince : compute the hours, minutes and seconds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFsgnchGI0Kb"
      },
      "source": [
        "BOS_token = 0 # Beginning Of Sentence token\n",
        "EOS_token = 1 # End Of Sentence token\n",
        "UNK_token = 2 # UNKnown token\n",
        "\n",
        "### Define the util functions\n",
        "def ids2words(dict_map, raw_data, sep=' ', eos_id=0, unk_sym='<unk>'):\n",
        "    str_text = ''\n",
        "    raw_data = raw_data.squeeze().tolist()\n",
        "\n",
        "    # Make the dict to inverse for translate unique number to word\n",
        "    dict_map_inv = dict()\n",
        "    for kk, vv in dict_map.items():\n",
        "        dict_map_inv[vv] = kk\n",
        " \n",
        "    for vv in raw_data:\n",
        "        if vv == eos_id:\n",
        "            break\n",
        "        if vv in dict_map_inv:\n",
        "            str_text = str_text + sep + dict_map_inv[vv]\n",
        "        else:\n",
        "            str_text = str_text + sep + unk_sym\n",
        "    return str_text.strip()\n",
        "\n",
        "def timeSince(since):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "\n",
        "  h = math.floor(s / 3600)\n",
        "  m = math.floor((s-3600*h) / 60)\n",
        "  s = s - h*3600 - m*60\n",
        "\n",
        "  return '{}h {}m {:.3f}s'.format(h,m,s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2HaqYVu92Pi"
      },
      "source": [
        "define your Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaMyddmDLZ3g"
      },
      "source": [
        "### Make my Language Model\n",
        "class LM(nn.Module):\n",
        "    def __init__(self, dict_len, dim_enc, dim_wemb, device):\n",
        "        super(LM, self).__init__()\n",
        "        self.dim_enc = dim_enc\n",
        "        self.wemb = dim_wemb\n",
        "        self.dict_len = dict_len\n",
        "        self.device = device\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # define embedding layer\n",
        "        # self.src_emb = ?\n",
        "        \n",
        "        # define LSTM layer\n",
        "        # self.rnn_enc = ?\n",
        "\n",
        "        # define hidden states to word embedding layer\n",
        "        # self.readout = ?\n",
        "\n",
        "        # define word embedding to raw word layer\n",
        "        # self.dec = ?\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, data, mask=None): \n",
        "        # data : (Timeseq, Batch)\n",
        "        x_data = data[:-1] # input (the last word is not the input)\n",
        "        y_data = data[1:]  # label (the first word is not the label)\n",
        "        if mask is not None:\n",
        "            x_mask = mask[1:]\n",
        "            y_mask = mask[1:]\n",
        "\n",
        "        Tx, Bn = x_data.size()\n",
        "       \n",
        "        # Word Embedding operation\n",
        "        # -----------------------\n",
        "        # x_emb = ? (embedding layer after flatten)\n",
        "        \n",
        "        x_emb = x_emb.view(Tx,Bn,-1)\n",
        "        x_emb = self.dropout(x_emb)\n",
        "        # x_emb : (Timeseq, Batch, dim_wemb)\n",
        "        \n",
        "        ht = torch.zeros(Bn,self.dim_enc)\n",
        "        ct = torch.zeros(Bn,self.dim_enc)\n",
        "        ht = Variable(ht).to(self.device)\n",
        "        ct = Variable(ct).to(self.device)\n",
        "        \n",
        "        gen_sentence = x_data[0].unsqueeze(1) # (Batch, 1)\n",
        "        loss = 0\n",
        "        for i in range(Tx):\n",
        "            # LSTM operation\n",
        "            # ----------------------\n",
        "            # ht, ct = ? (input: x_t, (ht-1, ct-1))\n",
        "            \n",
        "            # hidden states to word embedding operaiton\n",
        "            # ----------------------\n",
        "            # output = ?\n",
        "\n",
        "            output = self.dropout(output)\n",
        "            # output : (Batch, dim_wemb)\n",
        "\n",
        "            # word embedding to logit operaiton\n",
        "            # ----------------------\n",
        "            # logit = ?\n",
        "\n",
        "            # Compute the loss\n",
        "            # ----------------------\n",
        "            # loss = ? (input : logit, label_t)\n",
        "            \n",
        "            probs = self.softmax(logit)\n",
        "            topv, yt = probs.topk(1) # Choose top 1 prob. word\n",
        "           \n",
        "            gen_sentence = torch.cat((gen_sentence, yt), dim=1)\n",
        "            if mask is not None:\n",
        "                loss += torch.sum(loss_tmp*y_mask[i])/Bn\n",
        "            else:\n",
        "                loss += torch.sum(loss_tmp)/Bn\n",
        "        \n",
        "        return loss, gen_sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfiPrNQONsgW"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    \n",
        "    loss_total = 0\n",
        "    den = 0\n",
        "    for batch_idx, (data, mask) in enumerate(train_loader):\n",
        "        data, mask = torch.transpose(data,1,0).to(device), torch.transpose(mask,1,0).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss, gen_sentence = model(data, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        " \n",
        "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
        "                epoch, loss.item()))\n",
        "    real_sen = ids2words(src_dict, data[:,0], eos_id=EOS_token)\n",
        "    gen_sen = ids2words(src_dict, gen_sentence[0], eos_id=EOS_token)\n",
        "    print(\"train real sentence: {}\".format(real_sen))\n",
        "    print(\"train gen. sentence: {}\".format(gen_sen))\n",
        "    print(\"========================================\")        \n",
        "    return loss.item(), gen_sentence\n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, mask) in enumerate(test_loader):\n",
        "            data, mask = torch.transpose(data,1,0).to(device), torch.transpose(mask,1,0).to(device)\n",
        "            data, mask = data.to(device), mask.to(device)\n",
        "            loss, gen_sentence = model(data)\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= batch_idx\n",
        "\n",
        "    print('\\nTest: Average loss: {:.4f}\\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2AVB4B3LwqR"
      },
      "source": [
        "define your own custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQghL1vv1RDz"
      },
      "source": [
        "### Make my custom Dataset and DatasetLoader classes\n",
        "class ptb_dataset(Dataset):\n",
        "    def __init__(self,train_data,data_dict,maxlen=30):\n",
        "        # Load the dataset and word_dict\n",
        "        self.train_data_raw = open(train_data, 'r')\n",
        "        with open(data_dict, 'rb') as f:\n",
        "            self.data_dict = pkl.load(f)\n",
        "\n",
        "        # Make dict has unique index\n",
        "        self.data_dict2 = dict()\n",
        "        for kk, vv in self.data_dict.items():\n",
        "            self.data_dict2[kk] = vv + 1\n",
        "        self.data_dict2['<s>'] = BOS_token\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "        # Pre-processing the datasets\n",
        "        self.train_data, self.train_len = self.data_init(self.train_data_raw)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.train_data[index,:self.train_len[index]]\n",
        "        x_data, x_mask = self.prepare_text(sentence)\n",
        "        return torch.tensor(x_data).type(torch.long),\\\n",
        "               torch.tensor(x_mask).type(torch.float)\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.train_data)\n",
        "\n",
        "    def dict_len(self):\n",
        "        return len(self.data_dict2)\n",
        "    \n",
        "    def use_dict(self):\n",
        "        return self.data_dict2\n",
        "\n",
        "    def data_init(self, data):\n",
        "        #Check the number of sample < maxlen\n",
        "        # --------------------------\n",
        "        num = 0\n",
        "        while True:\n",
        "            # ? (read one dataline)\n",
        "            if sentence == \"\":\n",
        "                break\n",
        "            # ? (check the number of dataline that is not over the maxlen)\n",
        "              \n",
        "        # Make the preprocessed dataset\n",
        "        dataset = np.zeros((num, self.maxlen))\n",
        "        data_len = np.zeros(num, dtype=np.int)\n",
        "        idx = 0\n",
        "        data.seek(0)\n",
        "        while True:\n",
        "            # Pre-process the raw dataset\n",
        "            # -------------------------\n",
        "            # ? (read one dataline)\n",
        "            if sentence == \"\":\n",
        "                break\n",
        "            # sentence = ? (splitting the sentence)\n",
        "\n",
        "            # sentence = ? (tokenize sentence that is not over the maxlen)\n",
        "\n",
        "                dataset[idx,:len(sentence)] = sentence\n",
        "                data_len[idx] = len(sentence)\n",
        "                idx += 1\n",
        "        return dataset, data_len\n",
        "    \n",
        "    def prepare_text(self, sentence):\n",
        "        maxlen = self.maxlen + 2 # +2 for BOS and EOS\n",
        "        x_data = np.ones(maxlen).astype('int64')\n",
        "        x_mask = np.zeros(maxlen).astype('float32')\n",
        "        # prepare the data sample\n",
        "        # -----------------------\n",
        "        # x_data = ?\n",
        "        # x_data = ? (BOS token)\n",
        "        # x_mask = ?\n",
        "        return x_data, x_mask    \n",
        "\n",
        "def ptb_loader(train_data, batch_size, maxlen):\n",
        "    data_dict = 'drive/My Drive/public/data/ptb/ptb.train.txt.pkl'\n",
        "\n",
        "    data = ptb_dataset(train_data, data_dict, maxlen=maxlen)\n",
        "    src_dict = data.use_dict()\n",
        "    \n",
        "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return data_loader, data.dict_len(), src_dict\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vziddcLM_Wbu"
      },
      "source": [
        "### Test my dataset, datasetloader classes\n",
        "batch_size = 1\n",
        "maxlen = 30\n",
        "\n",
        "train_data = 'drive/My Drive/public/data/ptb/ptb.train.txt'\n",
        "train_loader, dict_len, src_dict = ptb_loader(train_data, batch_size, maxlen)\n",
        "\n",
        "for i, (x_data, x_mask) in enumerate(train_loader):\n",
        "    real_sen = ids2words(src_dict, x_data, eos_id=EOS_token)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\"real sentence: \")\n",
        "    print(real_sen)\n",
        "    print(\"x_data.shape: \", x_data.shape)\n",
        "    print(\"x_data:\")\n",
        "    print(x_data)\n",
        "    print(\"x_mask:\")\n",
        "    print(x_mask)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    if i >= 5:\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qb3mr1VoM21"
      },
      "source": [
        "### Hyperparameters\n",
        "batch_size = 64\n",
        "test_batch_size=1\n",
        "maxlen = 30\n",
        "dim_enc = 400\n",
        "dim_emb = 300\n",
        "lr = 0.0001\n",
        "optimizer = 'Adam'\n",
        "max_epoch = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ujSD18OK-u"
      },
      "source": [
        "### Training\n",
        "train_data = 'drive/My Drive/public/data/ptb/ptb.train.txt'\n",
        "test_data = 'drive/My Drive/public/data/ptb/ptb.valid.txt'\n",
        "\n",
        "# Check the device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# build my dataset loader\n",
        "train_loader, dict_len, src_dict = ptb_loader(train_data, batch_size, maxlen)\n",
        "test_loader, _, _ = ptb_loader(test_data, test_batch_size, maxlen)\n",
        "\n",
        "# build my model\n",
        "model = LM(dict_len, dim_enc, dim_emb, device)\n",
        "model.to(device)\n",
        "\n",
        "# build the optimizer\n",
        "if optimizer == 'RMSprop':\n",
        "    opt = optim.RMSprop(model.parameters(), lr=lr)\n",
        "elif optimizer == 'Adam':\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "elif optimizer == 'Adadelta':\n",
        "    opt = optim.Adadelta(model.parameters(), lr=lr)\n",
        "else:\n",
        "    opt = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Training..\n",
        "print(\"Training Start!\")\n",
        "best_loss = 99999\n",
        "for epoch in range(max_epoch):\n",
        "    train(model, device, train_loader, opt, epoch, log_interval)\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    if best_loss > test_loss:\n",
        "        print(\"We found the best model!\")\n",
        "        best_loss = test_loss\n",
        "        save_dir = 'drive/My Drive/public/results/ptb_trained_model_best.pth'\n",
        "        if os.path.exists(save_dir):\n",
        "            os.remove(save_dir)\n",
        "        torch.save(model, save_dir)\n",
        "\n",
        "print(\"Training is done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUpFs90x7sxC"
      },
      "source": [
        "**The End!!**"
      ]
    }
  ]
}