{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRM2Yy5nyviX"
      },
      "source": [
        "**Sentiment Analysis**\n",
        "\n",
        "---\n",
        "\n",
        "Most codes, explanations, and figures are from (Ben Trevett/ Pytorch Sentiment Analysis)\n",
        "\n",
        "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKsg0M-jzY9s"
      },
      "source": [
        "Mount your google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B123iPA_yrVh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIBNJIAqzess"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoobeDBxzhsV"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em6FrmZNzgtp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "from torchtext.legacy import data, datasets\n",
        "import random\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Kz2Ldr0Q6P"
      },
      "source": [
        "Define sentiment analysis model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VleuYchx0OYE"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim,\\\n",
        "                dropout, rnn_type, bidirectional, n_layers):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.bidir = bidirectional\n",
        "        if self.bidir == True:\n",
        "            bi_coeff = 2\n",
        "        else:\n",
        "            bi_coeff = 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        if rnn_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, \\\n",
        "                              bidirectional=bidirectional)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\\\n",
        "                               bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(bi_coeff*hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, text, label):\n",
        "        # text (Timeseq, Batch)\n",
        "        Tx, Bn = text.size()\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded (Timeseq, Batch, emb_dim)\n",
        "\n",
        "        if self.rnn_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded)\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        if self.bidir == True:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "        # hidden (Batch, hidden_dim*direction)\n",
        "        \n",
        "        logit = self.fc(hidden)\n",
        "        # logit (Batch, output_dim)\n",
        "\n",
        "        if label is not None:\n",
        "            loss = self.criterion(logit, label)\n",
        "        else:\n",
        "            loss = 0\n",
        "        probs = self.softmax(logit)\n",
        "        # probs (Batch, output_dim)\n",
        "\n",
        "        return loss, probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKtqJdBI0ypt"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    correct = 0\n",
        "    den = 0\n",
        "    train_loss = 0\n",
        "    for batch_idx, (batch) in enumerate(train_loader):\n",
        "        den += 1\n",
        "        text = batch.text.to(device)\n",
        "        label = batch.label.to(device)\n",
        "              \n",
        "        optimizer.zero_grad()\n",
        "        loss, probs = model(text, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(probs, dim=1)\n",
        "        correct += (pred == label).float().sum().item()\n",
        "        train_loss += loss.item()\n",
        "  \n",
        "        Tx, Bn = text.size()\n",
        "        \n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    train_loss /= den\n",
        "    print('Epoch {} Train: Loss: {:.6f} \\tAcc.: {:.6f}'.format(\n",
        "                epoch, train_loss, acc))   \n",
        "    \n",
        "            \n",
        "def test(model, device, test_loader, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    den = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            den += 1\n",
        "            text = batch.text.to(device)\n",
        "            label = batch.label.to(device)\n",
        "            \n",
        "            loss, probs = model(text, label)\n",
        "            pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "            correct += (pred == label).float().sum()\n",
        "\n",
        "    test_loss /= den\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Epoch {} Test : Loss: {:.6f} \\tAcc.: {:.6f}\\n'.format(\n",
        "        epoch, test_loss, acc))\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ8bQgWE0UJO"
      },
      "source": [
        "Download the IMDB dataset and make data iterators, vocabulary sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNZ-b9GxHa4k"
      },
      "source": [
        "# Tokenizing whole dataset once and save it!\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "print(\"Downloading and tokenizing\")\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "print(\"Make dataset\")\n",
        "train_examples = [vars(t) for t in train_data]\n",
        "test_examples = [vars(t) for t in test_data]\n",
        "\n",
        "print(\"Storing\")\n",
        "if not os.path.exists('./drive/My Drive/public/data/imdb'):\n",
        "    os.mkdir('./drive/My Drive/public/data/imdb')\n",
        "\n",
        "with open('./drive/My Drive/public/data/imdb/sentiment_train.json', 'w+') as f:\n",
        "    for example in train_examples:\n",
        "        json.dump(example, f)\n",
        "        f.write('\\n')\n",
        "        \n",
        "with open('./drive/My Drive/public/data/imdb/sentiment_test.json', 'w+') as f:\n",
        "    for example in test_examples:\n",
        "        json.dump(example, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noatx7fw0ThA"
      },
      "source": [
        "TEXT = data.Field()\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path = './drive/My Drive/public/data/imdb',\n",
        "    train = 'sentiment_train.json',\n",
        "    test = 'sentiment_test.json',\n",
        "    format = 'json',\n",
        "    fields = fields\n",
        ")\n",
        "\n",
        "print(\"Splitting for train and validation\")\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(1234))\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "print(\"Generating vocabulary sets\")\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz2O1osk02XN"
      },
      "source": [
        "Check the dataset and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyNaEBuR0-mX"
      },
      "source": [
        "print(\"Number of training data : \", len(train_data))\n",
        "print(\"Number of validation data : \", len(valid_data))\n",
        "print(\"Number of test data : \", len(test_data))\n",
        "\n",
        "print(\"Number of unique tokens of data vocab. : \", len(TEXT.vocab))\n",
        "print(\"Number of unique tokens of label vocab. : \", len(LABEL.vocab))\n",
        "\n",
        "print(\"One of the training example\")\n",
        "print(vars(train_data.examples[0]))\n",
        "\n",
        "print(\"Vocabulary set\")\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThgRK8Wl0_a2"
      },
      "source": [
        "Set your hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djRPS4E601hj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 2\n",
        "DROPOUT = 0.5\n",
        "\n",
        "LR = 0.001\n",
        "OPTIMIZER = 'Adam'\n",
        "MAX_EPOCH = 5\n",
        "\n",
        "RNN_TYPE = 'rnn'\n",
        "PRETRAIN_EMBEDDING = 0\n",
        "BIDIRECTIONAL = False\n",
        "N_LAYERS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YebRK6171E41"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGP8ze91EVr"
      },
      "source": [
        "### Training\n",
        "# Check the device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# Datasets\n",
        "train_loader = data.BucketIterator(train_data, batch_size = BATCH_SIZE)\n",
        "valid_loader = data.BucketIterator(valid_data, batch_size = BATCH_SIZE)\n",
        "test_loader = data.BucketIterator(test_data, batch_size = BATCH_SIZE)\n",
        "\n",
        "# build my model\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, RNN_TYPE, BIDIRECTIONAL, N_LAYERS)\n",
        "if PRETRAIN_EMBEDDING == 1:\n",
        "    pretrained_embeddings = TEXT.vocab.vectors\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.to(device)\n",
        "\n",
        "# build the optimizer\n",
        "if OPTIMIZER == 'RMSprop':\n",
        "    opt = optim.RMSprop(model.parameters(), lr=LR)\n",
        "elif OPTIMIZER == 'Adam':\n",
        "    opt = optim.Adam(model.parameters(), lr=LR)\n",
        "elif OPTIMIZER == 'Adadelta':\n",
        "    opt = optim.Adadelta(model.parameters(), lr=LR)\n",
        "else:\n",
        "    opt = optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "# Training..\n",
        "print(\"Training Start!\")\n",
        "best_acc = 0\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    train(model, device, train_loader, opt, epoch)\n",
        "    valid_acc = test(model, device, valid_loader, epoch)\n",
        "\n",
        "    if best_acc < valid_acc:\n",
        "        print(\"We found the best model!\\n\")\n",
        "        best_acc = valid_acc\n",
        "        save_dir = 'drive/My Drive/public/results/sentiment_analysis_model_best.pth'\n",
        "        if os.path.exists(save_dir):\n",
        "            os.remove(save_dir)\n",
        "        torch.save(model, save_dir)\n",
        "    \n",
        "print(\"Training is done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9nNcm9ChuhW"
      },
      "source": [
        "Test model on your own sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy6vBtlrhh8M"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    _, probs = model(tensor, None)\n",
        "\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Ww_cC3htlT"
      },
      "source": [
        "probs = predict_sentiment(model, \"This film is Wonderful\")\n",
        "print(probs) #[neg, pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19MG4fOcxiA7"
      },
      "source": [
        "  **The End!!**"
      ]
    }
  ]
}