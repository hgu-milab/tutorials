{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Most of sources are from https://github.com/bentrevett/pytorch-sentiment-analysis"
      ],
      "metadata": {
        "id": "CQW99v_8Zfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-cTBWnMRqE4",
        "outputId": "75ee3070-3e22-46c7-e193-36a25ae074b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchtext==0.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKctd6CqPq3a",
        "outputId": "2b30a01d-d637-4b74-ebe3-8534ff3d20b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu111)\n",
            "Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.7/dist-packages (0.9.0+cu111)\n",
            "Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0) (1.8.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "from torchtext.legacy import data, datasets\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "bAtKcQV-RDcn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing whole dataset once and save it!\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "print(\"Downloading and tokenizing\")\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "print(\"Make dataset\")\n",
        "train_examples = [vars(t) for t in train_data]\n",
        "test_examples = [vars(t) for t in test_data]\n",
        "\n",
        "print(\"Storing\")\n",
        "if not os.path.exists('./drive/My Drive/public/data/imdb'):\n",
        "    os.mkdir('./drive/My Drive/public/data/imdb')\n",
        "\n",
        "with open('./drive/My Drive/public/data/imdb/sentiment_train.json', 'w+') as f:\n",
        "    for example in train_examples:\n",
        "        json.dump(example, f)\n",
        "        f.write('\\n')\n",
        "        \n",
        "with open('./drive/My Drive/public/data/imdb/sentiment_test.json', 'w+') as f:\n",
        "    for example in test_examples:\n",
        "        json.dump(example, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvHBvjKuRASh",
        "outputId": "7530ac9b-2324-4a03-9ce4-7a179f1ded8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and tokenizing\n",
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.92MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Make dataset\n",
            "Storing\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field()\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "    path = './drive/My Drive/public/data/imdb',\n",
        "    train = 'sentiment_train.json',\n",
        "    test = 'sentiment_test.json',\n",
        "    format = 'json',\n",
        "    fields = fields\n",
        ")\n",
        "\n",
        "print(\"Splitting for train and validation\")\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(1234))\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "print(\"Generating vocabulary sets\")\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ztt73Y_SNyo",
        "outputId": "5e19ebfd-8638-4d63-a85f-571dba98510e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting for train and validation\n",
            "Generating vocabulary sets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.34MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29304.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training data : \", len(train_data))\n",
        "print(\"Number of validation data : \", len(valid_data))\n",
        "print(\"Number of test data : \", len(test_data))\n",
        "\n",
        "print(\"Number of unique tokens of data vocab. : \", len(TEXT.vocab))\n",
        "print(\"Number of unique tokens of label vocab. : \", len(LABEL.vocab))\n",
        "\n",
        "print(\"One of the training example\")\n",
        "print(vars(train_data.examples[0]))\n",
        "\n",
        "print(\"Vocabulary set\")\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzcKVaiZTI_-",
        "outputId": "32c277c7-a2d5-4586-978b-8fa901da5428"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data :  17500\n",
            "Number of validation data :  7500\n",
            "Number of test data :  25000\n",
            "Number of unique tokens of data vocab. :  25002\n",
            "Number of unique tokens of label vocab. :  2\n",
            "One of the training example\n",
            "{'text': ['We', 'expected', 'something', 'great', 'when', 'we', 'went', 'to', 'see', 'this', 'bomb', '.', 'It', 'is', 'basically', 'a', 'Broadway', 'play', 'put', 'on', 'film', '.', 'The', 'music', 'is', 'plain', 'terrible', '.', 'There', 'is', \"n't\", 'one', 'memorable', 'song', 'in', 'the', 'movie', '--', 'heard', 'any', 'hits', 'from', 'this', 'movie', '?', 'You', 'wo', \"n't\", 'because', 'there', 'are', \"n't\", 'any', '.', 'Some', 'of', 'the', 'musical', 'numbers', 'go', 'on', 'so', 'long', 'that', 'I', 'got', 'up', 'to', 'go', 'to', 'the', 'restroom', 'and', 'get', 'some', 'pop', 'corn', 'and', 'it', 'was', 'still', 'going', 'when', 'I', 'got', 'back', '!', 'If', 'they', 'were', 'good', 'songs', 'well', '--', 'but', 'they', 'suck', '.', 'The', 'pace', 'is', 'slow', ',', 'terrible', 'character', 'development', '.', 'The', 'lead', 'was', 'praised', 'for', 'her', 'singing', 'but', 'sounded', 'like', 'she', 'screamed', 'every', 'song', '--', 'it', 'was', 'almost', 'impossible', 'to', 'stand', '.', 'This', 'movie', 'has', 'NOTHING', 'to', 'offer', 'anyone', 'but', 'die', '-', 'hard', 'Broadway', 'enthusiasts', '.', 'This', 'is', 'without', 'a', 'doubt', 'the', 'most', 'over', 'rated', 'movie', 'I', \"'ve\", 'seen', 'in', 'my', 'entire', 'life', '.', 'A', 'complete', 'waist', 'of', 'time', 'and', 'money', '.', 'There', 'is', 'nothing', 'memorable', 'about', 'this', 'movie', 'except', 'Danny', 'Glover', '--', 'who', 'was', \"n't\", 'on', 'screen', 'enough', 'and', 'whose', 'character', 'was', \"n't\", 'developed', 'enough', '.', 'Rent', 'the', 'video', 'and', 'you', \"'ll\", 'agree', '--', 'this', 'movie', 'was', 'an', 'expensive', ',', 'over', 'produced', ',', 'polished', 'dog', 'do', '.'], 'label': 'neg'}\n",
            "Vocabulary set\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 2\n",
        "DROPOUT = 0.5\n",
        "\n",
        "LR = 0.001\n",
        "OPTIMIZER = 'Adam'\n",
        "MAX_EPOCH = 5\n",
        "\n",
        "RNN_TYPE = 'lstm'\n",
        "PRETRAIN_EMBEDDING = 0\n",
        "BIDIRECTIONAL = True\n",
        "N_LAYERS = 1"
      ],
      "metadata": {
        "id": "Chf3lCsiTNtD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim,\\\n",
        "                dropout, rnn_type, bidirectional, n_layers):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.bidir = bidirectional\n",
        "        if self.bidir == True:\n",
        "            bi_coeff = 2\n",
        "        else:\n",
        "            bi_coeff = 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        if rnn_type == 'rnn':\n",
        "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, \\\n",
        "                              bidirectional=bidirectional)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\\\n",
        "                               bidirectional=bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(bi_coeff*hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, text, label):\n",
        "        # text (Timeseq, Batch)\n",
        "        Tx, Bn = text.size()\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded (Timeseq, Batch, emb_dim)\n",
        "\n",
        "        if self.rnn_type == 'rnn':\n",
        "            output, hidden = self.rnn(embedded)\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        if self.bidir == True:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "        # hidden (Batch, hidden_dim*direction)\n",
        "        \n",
        "        logit = self.fc(hidden)\n",
        "        # logit (Batch, output_dim)\n",
        "\n",
        "        if label is not None:\n",
        "            loss = self.criterion(logit, label)\n",
        "        else:\n",
        "            loss = 0\n",
        "        probs = self.softmax(logit)\n",
        "        # probs (Batch, output_dim)\n",
        "\n",
        "        return loss, probs"
      ],
      "metadata": {
        "id": "z5f7176TTS1p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    correct = 0\n",
        "    den = 0\n",
        "    train_loss = 0\n",
        "    for batch_idx, (batch) in enumerate(train_loader):\n",
        "        den += 1\n",
        "        text = batch.text.to(device)\n",
        "        label = batch.label.to(device)\n",
        "              \n",
        "        optimizer.zero_grad()\n",
        "        loss, probs = model(text, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(probs, dim=1)\n",
        "        correct += (pred == label).float().sum().item()\n",
        "        train_loss += loss.item()\n",
        "  \n",
        "        Tx, Bn = text.size()\n",
        "        \n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    train_loss /= den\n",
        "    print('Epoch {} Train: Loss: {:.6f} \\tAcc.: {:.6f}'.format(\n",
        "                epoch, train_loss, acc))   \n",
        "    \n",
        "            \n",
        "def test(model, device, test_loader, epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    den = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            den += 1\n",
        "            text = batch.text.to(device)\n",
        "            label = batch.label.to(device)\n",
        "            \n",
        "            loss, probs = model(text, label)\n",
        "            pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "            correct += (pred == label).float().sum()\n",
        "\n",
        "    test_loss /= den\n",
        "    acc = correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Epoch {} Test : Loss: {:.6f} \\tAcc.: {:.6f}\\n'.format(\n",
        "        epoch, test_loss, acc))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "PSzIQCmETUhG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training\n",
        "# Check the device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# Datasets\n",
        "train_loader = data.BucketIterator(train_data, batch_size = BATCH_SIZE)\n",
        "valid_loader = data.BucketIterator(valid_data, batch_size = BATCH_SIZE)\n",
        "test_loader = data.BucketIterator(test_data, batch_size = BATCH_SIZE)\n",
        "\n",
        "# build my model\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, RNN_TYPE, BIDIRECTIONAL, N_LAYERS)\n",
        "if PRETRAIN_EMBEDDING == 1:\n",
        "    pretrained_embeddings = TEXT.vocab.vectors\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.to(device)\n",
        "\n",
        "# build the optimizer\n",
        "if OPTIMIZER == 'RMSprop':\n",
        "    opt = optim.RMSprop(model.parameters(), lr=LR)\n",
        "elif OPTIMIZER == 'Adam':\n",
        "    opt = optim.Adam(model.parameters(), lr=LR)\n",
        "elif OPTIMIZER == 'Adadelta':\n",
        "    opt = optim.Adadelta(model.parameters(), lr=LR)\n",
        "else:\n",
        "    opt = optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "# Training..\n",
        "print(\"Training Start!\")\n",
        "best_acc = 0\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    train(model, device, train_loader, opt, epoch)\n",
        "    valid_acc = test(model, device, valid_loader, epoch)\n",
        "\n",
        "    if best_acc < valid_acc:\n",
        "        print(\"We found the best model!\\n\")\n",
        "        best_acc = valid_acc\n",
        "        save_dir = 'drive/My Drive/public/results/sentiment_analysis_model_best.pth'\n",
        "        if os.path.exists(save_dir):\n",
        "            os.remove(save_dir)\n",
        "        torch.save(model, save_dir)\n",
        "    \n",
        "print(\"Training is done!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mvWro6sTYsB",
        "outputId": "49624417-1d09-4dbb-9089-d27cd6e2c27d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Start!\n",
            "Epoch 0 Train: Loss: 0.684666 \tAcc.: 0.548457\n",
            "Epoch 0 Test : Loss: 0.660621 \tAcc.: 0.606133\n",
            "\n",
            "We found the best model!\n",
            "\n",
            "Epoch 1 Train: Loss: 0.651532 \tAcc.: 0.618057\n",
            "Epoch 1 Test : Loss: 0.603748 \tAcc.: 0.682000\n",
            "\n",
            "We found the best model!\n",
            "\n",
            "Epoch 2 Train: Loss: 0.639997 \tAcc.: 0.634743\n",
            "Epoch 2 Test : Loss: 0.660230 \tAcc.: 0.680133\n",
            "\n",
            "Epoch 3 Train: Loss: 0.610668 \tAcc.: 0.674571\n",
            "Epoch 3 Test : Loss: 0.571602 \tAcc.: 0.728533\n",
            "\n",
            "We found the best model!\n",
            "\n",
            "Epoch 4 Train: Loss: 0.550464 \tAcc.: 0.726971\n",
            "Epoch 4 Test : Loss: 0.491066 \tAcc.: 0.780000\n",
            "\n",
            "We found the best model!\n",
            "\n",
            "Training is done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    _, probs = model(tensor, None)\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "6-sOemSXTrSI",
        "outputId": "503dd967-764a-40fd-c947-1fddd94da93f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = predict_sentiment(model, \"This film is Wonderful\")\n",
        "print(probs) #[neg, pos]"
      ],
      "metadata": {
        "id": "pYbMI7WaTtW3",
        "outputId": "a6b6c51c-19b9-4c76-ebd8-795044c66cc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3132, 0.6868]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ]
        }
      ]
    }
  ]
}