{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10ColabExample_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsih6nZDbW3c"
      },
      "source": [
        "**Training CNN with cifar10 Dataset**\n",
        "\n",
        "\n",
        "---\n",
        "The data and results will be stored in the following directories\n",
        "\n",
        "  1. drive/My Drive/public/data/ has data\n",
        "  2. drive/My Drive/public/results/ will have results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFOrTQnwXkC0"
      },
      "source": [
        "Mount your good drive. Check by '! ls' command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpJ1eC5cVCO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ha4THMCFkYz"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhX-EncxXnVb"
      },
      "source": [
        "Import PyTorch library and check by printing the version information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejjXJNArc7af"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRjnQFfaEZUF"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lzWp7RyYaP4"
      },
      "source": [
        "Define your network model. We have defined a CNN model in advance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j66zCObdQE9"
      },
      "source": [
        "class CIFAR10_CNN_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10_CNN_model,self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(3,16,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16,32,3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2), # 32 x 16 x 16 (batch_size width height)\n",
        "            \n",
        "            ## Practice 2, ex. 1: complete additional convolution layers and maxpool layer\n",
        "            \n",
        "            # add one conv layer\n",
        "            # nn.Conv2d(?),\n",
        "\n",
        "            # add activation function\n",
        "            # ?,\n",
        "\n",
        "            # add another conv layer \n",
        "            # nn.Conv2d(?),\n",
        "            \n",
        "            # add activation function\n",
        "            # ?,\n",
        "            \n",
        "            # add max pooling layer\n",
        "            # nn.MaxPool2d(?)\n",
        "\n",
        "            ## Practice 2, ex. 2: uncomment another two additional conv layers\n",
        "            \n",
        "            # nn.Conv2d(128,256,3,padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(256,256,3,padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        conv_size = self.get_conv_size((3,32,32))\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(conv_size,200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200,10)\n",
        "        )       \n",
        "\n",
        "    def get_conv_size(self, shape):\n",
        "        o = self.layer(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # Define forward function of the model\n",
        "\n",
        "        # obtain batch size\n",
        "        # batch_size, c, h, w = ?\n",
        "\n",
        "        # feed data through conv layers\n",
        "        # out = ?\n",
        "\n",
        "        # reshape the output of convolution layer for fully-connected layer\n",
        "        # out = ?\n",
        "\n",
        "        # feed data through feed-forward layer\n",
        "        # out = ?\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YXGOHWdC01h"
      },
      "source": [
        "# check model\n",
        "model = CIFAR10_CNN_model()\n",
        "print(model)\n",
        "\n",
        "print('=' * 90)\n",
        "\n",
        "# check forward()\n",
        "mytensor = torch.zeros((1,3,32,32))\n",
        "out = model(mytensor)\n",
        "print('output shape: ', out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_z_D-mJYc2y"
      },
      "source": [
        "Load MNIST datset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DldSjf_pdEkW"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "cifar_train = dset.CIFAR10(\"drive/My Drive/public/data/\", train=True, \n",
        "                           transform=transforms.ToTensor(), \n",
        "                           target_transform=None, download=True)\n",
        "cifar_test = dset.CIFAR10(\"drive/My Drive/public/data/\", train=False, \n",
        "                          transform=transforms.ToTensor(), \n",
        "                          target_transform=None, download=True)\n",
        "\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar_train,batch_size=batch_size, \n",
        "                                  shuffle=True,num_workers=2,drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(cifar_test,batch_size=batch_size, \n",
        "                                  shuffle=False,num_workers=2,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6veQvPPDWSs"
      },
      "source": [
        "print('train dataset: ', cifar_train.__getitem__(0)[0].size(), cifar_train.__len__())\n",
        "print('test dataset: ', cifar_test.__getitem__(0)[0].size(), cifar_test.__len__())\n",
        "\n",
        "print('=' * 90)\n",
        "\n",
        "for batch, (data, target) in enumerate(train_loader):\n",
        "  print('data shape: ', data.shape)\n",
        "  print('target shape: ', target.shape)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfJj8CKnbJIW"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "  model.train()\n",
        "  for batch_idx,(data,target) in enumerate(train_loader):\n",
        "\n",
        "    # send tensors to GPU\n",
        "    # data, target = ?\n",
        "    \n",
        "    # initialize optimizer\n",
        "    # ?\n",
        "\n",
        "    # put data into model\n",
        "    # output = ?\n",
        "\n",
        "    # compute loss\n",
        "    # loss = ?\n",
        "    \n",
        "    # backpropagate error using loss tensor\n",
        "    # ?\n",
        "\n",
        "    # update model parameter using optimizer\n",
        "    # ?\n",
        "    \n",
        "    if batch_idx % log_interval == 0:\n",
        "      ## remove break and uncomment below after completing above \n",
        "      break\n",
        "      #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "      #  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "      #  100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "\n",
        "      _,output_index = torch.max(output,1)  \n",
        "      total += target.size(0)\n",
        "      correct += (output_index == target).sum().float()\n",
        "    \n",
        "    print('\\nTest: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odiRQ9QSYhJr"
      },
      "source": [
        "Define hyper-parameters for training. Then define model and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4xzCxNBPudw"
      },
      "source": [
        "seed = 1\n",
        "learning_rate = 0.001\n",
        "num_epoch = 5\n",
        "log_interval=100\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CIFAR10_CNN_model().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqfO72tXY-T5"
      },
      "source": [
        "Train the model by running training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FWKcQmmdbCJ"
      },
      "source": [
        "for epoch in range(1, num_epoch + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "if not os.path.exists('drive/My Drive/public/results'):\n",
        "    os.mkdir('drive/My Drive/public/results') \n",
        "torch.save(model, 'drive/My Drive/public/results/cifar10_pretrained.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T21EvJ1cPqqv"
      },
      "source": [
        "the end!\n",
        "----"
      ]
    }
  ]
}