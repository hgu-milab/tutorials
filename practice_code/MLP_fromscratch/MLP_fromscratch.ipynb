{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_fromscratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4KIw4wFM+FiSp6KxwZuvP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"p7veR88nGv3H"},"source":["# Pseudo-code for MLP from scratch\n","by the ML2021-2 TA "]},{"cell_type":"markdown","metadata":{"id":"IcU-u43GIxCe"},"source":["Hello! Please read this document carefully. I hope it helps understand how MLP for a classification task works, and a general idea of how a neural network can 'learn'. I will start form the very basic intuition, and then build up from there until the actual implementation of the MLP.\n","\n","First, we will start from the task. We want to classify some images from the MNIST dataset. That is, we want a model that takes an image as an iput and reutrns the label.\n","\n","<div>\n","<img src=https://drive.google.com/uc?export=view&id=1KZzE6S5bs0p6IL0xZXX4FDInvmo_2Sfw width=\"500\"/>\n","</div>\n","\n","\n","Basically speaking, this is just a (veeeeery complex) function y = f(x), where f is replaced with the model.\n","\n","In our case, f(.)(the model) is the MLP network.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qy2-XJ_PNRXA"},"source":["## The input"]},{"cell_type":"markdown","metadata":{"id":"PJDwChWTNXV8"},"source":["The input to our model would be the images. As we have seen many times, this images are described by their pixel values. For the MNIST case (grey-scale 28x28 pixel images), we only need one value for each pixel, ranging between 0-255.\n","\n","Basically, our model should learn how each pixel value is related to the label of the images. The intuition is answering the question 'Why do this 784 pixel values represent a number 4?', for example.\n","\n","First step, we want to 'flatten' our images. Instead of an array (matrix) of 28x28, we will input a 784 vector. That is, we will make a vector by puting each pixel of the image one next to the other. For the MLP, this step is a computational convenience. For an illustrative example, refer to the image below:\n","\n","<div>\n","<img src=https://drive.google.com/uc?export=view&id=1lDznKUeVk-3neVJAC0qCchFFB6ZykFkp width=\"300\"/>\n","</div>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mG0rJDhp-wtj"},"source":["##Supervised learning and overall training\n","\n","This classification task is using supervised learning. This means that during training we can adjust our estimation according to the correct labels.\n","\n","The intuitive idea is as follows:\n","Given all the input images, our model will precdict their labels and store the 'reasoning' that led to this classification decision in the form of parameters (W, b). Then, we will compute a cost fucntion. This cost function is a summation of all the errors in our predictions w.r.t the true labels. Intuitively, we want this cost function to be as small as possible, since it will mean that we made little wrong predictions. The process described in this paragraph is called ***forward propagation***.\n","\n","But what if the classification that the model made is wrong for some labels? Given the cost function, we want to keep the 'reasoning' that led to a correct label, but change the 'resoning' that led to an incorrect label in such a way that next time, when we see that image, we will predict the correct label. To do so, we have to 'go back' in our reasoning process starting from this cost function and change the parameters that led to a wrong prediction. We do this via computing the gradients of all the variables and parameters that led to this cost function. This process is called ***backward propagation***. \n","\n","Finally, with the information of the gradients, we want to change the parameters a little in the direction that minimizes the cost function. There are several ways of updating the parameters, and this step is called optimization. Finally, after we change our parameters, we update the old ones by replacing them with these changed ones. It is like updating our 'reasoning'. \n","\n","After all this, we forward propagate all the images one more time with the new parameters and obtain a new classification. We compare our estimation with the true labels and backward propagate. We update our parameters. And repeat. \n","\n","To recapitulate, training an MLP classifier has three main parts:\n","\n","1. **Forward propagation:** classify all the images in the training set using the model and check the the predicted labels with the true labels using the cost function.\n","2. **Backward propagation:** go back in our reasoning and compute the derivatives of our cost function w.r.t. the parameters and variables we computed.\n","3. **Update:** change the parameter values that gave us the wrong labels in the direction that minimizes the error. Update the parameter values for the next forward propagation.\n","\n","Finally, when we trained our model to make the best possible predictions, we stop and save these parameters.\n","\n","To test if our model can generalize, we apply forward propagation to a new dataset (called test dataset) and we check the accuracy of our label predictions in this set. Why don't we backpropagate in this case? Because we are done training, we don't want to update the parameters any more with this test set (that is why is called test set ^^)."]},{"cell_type":"markdown","metadata":{"id":"U6LqwN5d58xR"},"source":["## MLP architecture\n","\n","Now we will talk about the MPL (our model) architecture. \n","The MLP architecture is made of:\n","\n","1.   **Input:** The input vector to our model (in this case, 784-dimensional vectors)\n","2.   **Hidden Layer(s):** a layer is a set of *k* neurons. Each neuron will capture information about each input pixel and store it in the parameters. We can have different layers with different number of neurons.\n","3. **Output Layer:** The last neuron that outputs the prediction (in this case, the labels)\n","\n","In this case, Professor said that we need only one hidden layer. So, our architecture will be like this: \n","\n","<div>\n","<img src=https://drive.google.com/uc?export=view&id=1EmTaFAmiFU3JAmkLddoyTvezbTjwM8kt width=\"500\"/>\n","</div>\n","\n","---\n","\n","**How many neurons should this hidden layer have?**\n","\n","The answer is: you have to choose. There is no rule to determine a correct number of neurons. \n","A common question is if this nerons should be the same number as the input image. They could be, but it is not mandatory. However, is it convenient? If we have too many neurons, we might capture too much useless information. But if we have too little amount, then we might fail to capture some details. The best strategy is to try different number of neurons. "]},{"cell_type":"markdown","metadata":{"id":"S1DNf97Zdc78"},"source":["##Let's code!\n","Now we are going to see each part in detail and also I will show a pseudo code for each one. If you understand correctly every step, you will be able to fill this colab. with the correct values"]},{"cell_type":"code","metadata":{"id":"8n1CC3AscD4U","executionInfo":{"status":"ok","timestamp":1640678028399,"user_tz":-540,"elapsed":707,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}}},"source":["#### Import your libraries (remember, no built-in library)\n","import six.moves.cPickle as pickle\n","import gzip\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pylab as plt\n","import random\n","from sklearn import metrics\n","import time"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7FQYYNwXcDiu"},"source":["Function for loading the data"]},{"cell_type":"code","metadata":{"id":"0s4tkJX6d_83"},"source":["def load_data(dataset):\n","    ''' Loads the dataset\n","\n","    :type dataset: string\n","    :param dataset: the path to the dataset (here MNIST)\n","    \n","    copied from http://deeplearning.net/ and revised by hchoi\n","    '''\n","\n","    # Download the MNIST dataset if it is not present\n","    data_dir, data_file = os.path.split(dataset)\n","    if data_dir == \"\" and not os.path.isfile(dataset):\n","        # Check if dataset is in the data directory.\n","        new_path = os.path.join(\n","            os.path.split(__file__)[0],\n","            dataset\n","        )\n","        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n","            dataset = new_path\n","\n","    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n","        from six.moves import urllib\n","        origin = (\n","            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n","        )\n","        print('Downloading data from %s' % origin)\n","        urllib.request.urlretrieve(origin, dataset)\n","\n","    print('... loading data')\n","\n","    # Load the dataset\n","    with gzip.open(dataset, 'rb') as f:\n","        try:\n","            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n","        except:\n","            train_set, valid_set, test_set = pickle.load(f)\n","    \n","    return train_set, valid_set, test_set\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9z6zMxtLeKis","executionInfo":{"status":"ok","timestamp":1637389357628,"user_tz":-540,"elapsed":1309,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}},"outputId":"aab0b6d0-8776-42a9-ebe3-aa3da8b4e5b3"},"source":["  train_set, val_set, test_set = load_data('./mnist.pkl.gz')\n","    \n","  ## I will not use a validation set, but you can if you want to :)\n","  ## train_x and test_x contain the images for training and testing. \n","  train_x, train_y = train_set\n","  test_x, test_y = test_set\n","  print('The amount of images in your training set is: ', train_x.shape[0])\n","  print('The amount of images in your testing set is: ', test_x.shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["... loading data\n","The amount of images in your training set is:  50000\n","The amount of images in your testing set is:  10000\n"]}]},{"cell_type":"markdown","metadata":{"id":"IesnS_fJfzFc"},"source":["## Pre-processing\n","Now, we have to pre-process the dataset. The first one is to scale our dataset by substracting the mean. This is called normalization and it helps the model to have inputs that have a similar scale of values."]},{"cell_type":"code","metadata":{"id":"pkyEND_WfwaR"},"source":["def norm(X):\n","    return X - np.mean(X, axis=0)\n","    \n","train_scaled = norm(train_x)\n","test_scaled = norm(test_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mfirNSNkgzTS"},"source":["Now, we will define a function to convert our labels into one-hot-encoded vectors. This is also convenient in terms of computation during our training. Instead of having the labels as integers (0,1,2,3,4...,9) we will write them as 10 dimensional vectors of 0s and 1s as follows:\n","\n","$0 = [1,0,0,0,0,0,0,0,0,0]$\n","\n","$1 = [0,1,0,0,0,0,0,0,0,0]$\n","\n","$2 = [0,0,1,0,0,0,0,0,0,0]$\n","\n",".\n",".\n",".\n","\n","$9 = [0,0,0,0,0,0,0,0,0,1]$\n","\n","Lucky for us, the libraries like pytorch and similar do this automatically for us, so this step is not necessary when we use them. However, for this part of the homework we have to do this manually (ㅠㅠ)"]},{"cell_type":"code","metadata":{"id":"jPhoCJqVicMP"},"source":["##We will use it later during training :)\n","def one_hot(y):\n","    one_hot = []\n","    oh = np.zeros(10, dtype=int)\n","    for i in y:\n","        oh[i] = 1\n","        one_hot.append(oh)\n","        oh = np.zeros(10, dtype=int)\n","    return np.asarray(one_hot)\n","\n","##One-hot encode for all the labels:\n","##Y = one_hot(train_y)\n","##print(Y, Y.shape)\n","##print('\\n')\n","##Let's try with one example!\n","##print('If the integer label is: ', train_y[2])\n","##print('its one-hot-vector is: ', Y[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHHOEZ_XpOsd"},"source":["## Model hyperparameters\n","\n","Now we are going to define the model hyperparameters and other needed values. Remember, hyperparamenters mean **non**-trainable parameters, so they will not change throughout our training."]},{"cell_type":"code","metadata":{"id":"Kd2YlxvApkd3"},"source":["#Input dimension\n","x_dim = 28 * 28 \n","\n","#Number of neurons in our only hidden layer\n","h = 200 \n","\n","#Number of classes (10 classes for us)\n","C = 10 \n","\n","#Learning rate (how much do we want to change the parameters when we update, see details below):\n","lr = 0.01\n","\n","#Epochs (How many times do we want to forward and backpropagate all our data?) \n","#Do not decrease this value (I tried this code and it needs a lot of epochs to train), but you can increase it if you want.\n","num_epochs=10000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBMF60zcrd-u"},"source":["## Initialization\n","\n","First, we need to define and initialize our parameters from the hidden layer and the ouput layer. We have 2 sets of parameter matrices ($W_{xh}$, $b_h$) for our only hidden layer and ($W_o$, $b_o$) for the output layer. \n","\n","Can you think of the shape of these before we compute them? As a hint, refer to the structure of MLP:\n","\n","<div>\n","<img src=https://drive.google.com/uc?export=view&id=1Is995KDQ486LkOHFi-R8A1RYmlYoKqCE width=\"500\"/>\n","</div>"]},{"cell_type":"code","metadata":{"id":"LRkRtv2nrdWo"},"source":["def initp(x_dim, h, C):\n","\n","    ##Initialize each array according to the corresponding dimension\n","    ## W@ are inistialized randomly, and b@ can be initialized as zero vectors (why can't W@ be zero?)\n","\n","    W_xh = np.random.randn(x_dim, ...) #complete the  ...\n","    b_h = np.zeros((1, ...))\n","    Wo = np.random.randn(h, ...) \n","    bo = np.zeros((1, ...))\n","    \n","    ##Save the parameter values as a dictionary. \n","    ##This is our 'memory' where we store our parameters, and later we update them here.\n","    parameters = {\"W_xh\": W_xh,\n","                  \"b_h\": b_h,\n","                  \"Wo\": Wo,\n","                  \"bo\": bo}\n","    \n","    return parameters\n","\n","#parameters = initp(x_dim, h, C) ## <- check the dimensions are correct!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZD_eEbV8Zmqj"},"source":["## Forward Propagation\n","\n","Now let's see forward propagation in more detail. From the slides in the ppt (slide 10), each layer will compute $y_t$ and an activation $h_t$. Pay attention to the subscripts. ALSO, I changed the notation a little bit to make it a little more clear, but the equations are the same as in the ppt.\n","\n","Each neuron in the hidden layer will take each pixel value from the input $X$ and multiply it by the weight matrix $W_{xh}$ and add the bias vector $b_h$. If we use matrix notation (so we don't have to use a for loop), this will be:\n","\n","$Z_1=W_{xh} * X + b_h$\n","\n","Then, we need to add the non-linearity through an activation function. See the pdf slides for some examples of activation functions. Again there is no rule to choose activation functions. For example, if we choose ReLU:\n","\n","$A_1=ReLU(Z_1)$\n","\n","This is it for the hidden layer. The input are the images $X$ as an array. The output is an array $A_1$.\n","\n","Now, this $A_1$ is the array that contains information about the image. This is the input to our next layer (which in our case is our final layer, the output layer.) In this layer we use $A_1$ to compute a new vector \n","\n","$Z_o=W_{ho} * A_1 + b_o$\n","\n","And now, we transform this vector into probabilities using the softmax function:\n","\n","$A_o=softmax(Z_o)$\n","\n","That's it! This vector will give the probabilities of the input image being a certain label (so ten probabilities for each image in our array $X$... now you see why we needed the one-hot encoded vector? ^^)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qM20miFdVX-x","executionInfo":{"status":"ok","timestamp":1637389398858,"user_tz":-540,"elapsed":912,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}},"outputId":"af67dda3-ba74-471f-df73-793b7849ac13"},"source":["##Let's do everything from scratch! even the softmax function :)\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0) \n","\n","def forward_prop(X, parameters):\n","    \n","    ##Let's load our current parameters\n","    W_xh = parameters[\"W_xh\"]\n","    b_h = parameters[\"b_h\"]\n","    Wo = parameters[\"Wo\"]\n","    bo = parameters[\"bo\"]\n","    \n","    ##This is basically the MLP:\n","    ##hidden layer:\n","    Z1 = np.dot(W_xh.T, X) + b_h.T\n","    A1 = your_act(Z1) ##replace your_act with an activation of your liking like ReLU or tanh\n","    ##output layer:\n","    Zo = np.dot(..., ...) + ... #complete the  ...\n","    Ao = softmax(...) #complete the  ...\n","\n","    ##we make a cache, that is a 'memory' so that later we can go back in our reasoning when we do back propagation\n","    cache = {\"Z1\": Z1,\n","             \"A1\": A1,\n","             \"Zo\": Zo,\n","             \"Ao\": Ao}\n","    \n","    ##the output is our probabilities. \n","    return Ao, cache\n","\n","##Check!!\n","#ao , cache = forward_prop(train_scaled.T, parameters)\n","#print(ao.shape, cache)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 50000) {'Z1': array([[-7.48417134, -9.74897675, -2.69714221, ..., -2.32342256,\n","         1.51079418, -8.87261142],\n","       [-5.5108277 , -5.84043228,  8.72033166, ...,  0.1108445 ,\n","        -6.82427461,  9.65278565],\n","       [ 2.06974446,  4.18607877, -7.83777494, ..., -6.65157498,\n","         1.79771686, -4.01848792],\n","       ...,\n","       [-1.33849355, -1.4438885 , -3.62665483, ...,  2.36729967,\n","        -5.42317423,  5.01208294],\n","       [ 3.04731579, -9.94127643, 12.97643931, ..., -0.99049326,\n","         3.81720559,  0.64652026],\n","       [ 3.18008206,  2.99445515,  2.72347061, ..., -4.19819751,\n","        -7.41945631,  7.73275125]]), 'A1': array([[-0.99999937, -0.99999999, -0.99095614, ..., -0.98099863,\n","         0.90707989, -0.99999996],\n","       [-0.99996731, -0.99998309,  0.99999995, ...,  0.11039276,\n","        -0.99999764,  0.99999999],\n","       [ 0.96863765,  0.99953767, -0.99999969, ..., -0.99999666,\n","         0.94656906, -0.99935364],\n","       ...,\n","       [-0.87130994, -0.89447778, -0.99858535, ...,  0.98258111,\n","        -0.99996105,  0.99991137],\n","       [ 0.99550027, -1.        ,  1.        , ..., -0.75757257,\n","         0.99903342,  0.56932277],\n","       [ 0.9965478 ,  0.99499975,  0.99141805, ..., -0.99954874,\n","        -0.99999928,  0.99999962]]), 'Zo': array([[  0.31943685,  12.27091011,  -7.61853223, ...,  -1.08202983,\n","         -6.19006999, -19.6070938 ],\n","       [ 22.7568496 ,  30.9360034 ,  -7.80794156, ..., -18.19135245,\n","         -0.54347225,  -6.21478557],\n","       [-17.15085198,  18.80388517, -21.87683377, ...,   0.9658444 ,\n","         -3.46682881,  15.28167024],\n","       ...,\n","       [  5.2348363 ,  14.02346549,  -8.05984408, ..., -22.53363573,\n","         -0.73917966, -21.71386455],\n","       [-14.41583578,  -1.83446886,   4.99742319, ...,  -3.51656527,\n","         -6.99880733,   9.465133  ],\n","       [ -1.37394976,  12.49080973,   8.18799298, ..., -18.24226248,\n","         14.39431612,   5.86043637]]), 'Ao': array([[1.80117234e-10, 7.83160191e-09, 3.44176944e-09, ...,\n","        8.20676399e-05, 1.14367051e-09, 5.26539429e-16],\n","       [9.99999579e-01, 9.99994532e-01, 2.84788435e-09, ...,\n","        3.04570291e-12, 3.24031451e-07, 3.44854949e-10],\n","       [4.65913437e-18, 5.38375918e-06, 2.21044695e-15, ...,\n","        6.36139586e-04, 1.74176494e-08, 7.47185789e-01],\n","       ...,\n","       [2.45632671e-08, 4.51830700e-08, 2.21371891e-09, ...,\n","        3.96148368e-14, 2.66435763e-07, 6.40430470e-17],\n","       [7.17972284e-17, 5.86087359e-15, 1.03710230e-03, ...,\n","        7.19228078e-06, 5.09413865e-10, 2.22504384e-03],\n","       [3.31227822e-11, 9.75779741e-09, 2.52039409e-02, ...,\n","        2.89452692e-12, 9.95373848e-01, 6.05116113e-05]])}\n"]}]},{"cell_type":"markdown","metadata":{"id":"xv-tHQ6DzTl4"},"source":["##The objective function (NLL)\n","\n","First we need to define our cost function, the one that will give us the overall error we made in our predictions. We are going to use the negative log likelihood (NLL). Refer to slides 19 and 20 as a guide (I will change the notation slightly because I will use matrices.)\n","\n","Let $Y$ be the true labels of our dataset and $A_o$ our estimation from the output layer in the MLP. For our $n$ samples we define the NLL as:\n","\n","$\\mathcal{L} = -\\frac{1}{n}\\sum_i^n Y_i * \\ln(A_{o(i)})$"]},{"cell_type":"code","metadata":{"id":"rup4dMWo0a-p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637389443576,"user_tz":-540,"elapsed":372,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}},"outputId":"664827de-9a10-4a7f-d64c-a6bbfa82dd1b"},"source":["\n","def nll_cost(Ao, Y, parameters):\n","\n","    n = Y.shape[0]\n","\n","    logprobs = np.multiply(np.log(Ao), Y.T)\n","    cost =  (-1./n)*np.sum(logprobs)\n","    cost = np.squeeze(cost)     \n","\n","    return cost\n","\n","##nll_cost(ao, one_hot(train_y), parameters) # <- check!"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19.030600548583934"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"rYwbkdGpcdpD"},"source":["##Backward propagation\n","\n","Refer to the slides from 21 onwards (gradients for multiclass). Again, the notation changed a little, but we are doing what the ppt says.\n","\n","Now we are going to revise our model from the output back to the input using the gradient of our loss (use the derivative chain rule!). \n","\n","We start with the cost function (we use matrix notation, so we need the term $\\frac{1}{n}$, for our $n$ samples):\n","\n","$\\mathcal{L} = -\\frac{1}{n}\\sum_i^n Y_i * \\ln(A_{o(i)})$\n","\n","and we compute the derivative $dZ_o = \\frac{d\\mathcal{L}}{dZ_o}$: \n","\n","$dZ_o = \\frac{\\partial \\mathcal{L}}{\\partial A_o} \\frac{\\partial \\mathcal{A_o}}{\\partial Z_o} = A_o - Y$\n","\n","We do the same for $dW_* = \\frac{d\\mathcal{L}}{dW_*}$, and $db_* = \\frac{d\\mathcal{L}}{db_*}$\n","\n","Compute these derivatives yourself using the chain rule (use the slides 21 and 22 as reference)"]},{"cell_type":"code","metadata":{"id":"3c4dZOm_chFL"},"source":["def back_prop(parameters, cache, X, Y):\n","    \n","    ##Amount of examples\n","    n = X.shape[0]\n","\n","    ##Load current parameter weights\n","    W_xh = parameters[\"W_xh\"]\n","    Wo = parameters[\"Wo\"]\n","    \n","    ##Load the activation information of each layer\n","    A1 = cache[\"A1\"]\n","    Ao = cache[\"Ao\"]\n","    \n","    ##Let's compute the derivatives! Note we are going backwards, from the output layer to the hidden layer\n","    dZo= Ao - Y.T\n","    #dim of dWo should be (10, 200)\n","    dWo = (1./n)*np.dot(..., ...) #complete the  ...\n","    #dim of dbo should be (10, 1)\n","    dbo = (1./n)*np.sum(..., axis=1, keepdims=True) #complete the  ...\n","    #dim of dZ1 should be (200, 50000)\n","    dZ1 = np.dot(dWo.T, dZo) * derivative_of_your_activation(...) #complete the information wrt the activation that you chose\n","    #dim of dW_xh should be (200, 784)\n","    dW_xh = (1./n)*np.dot(...) #complete the  ...\n","    #dim of db_h should be (200, 1)\n","    db_h = (1./n)*np.sum(..., axis=1, keepdims=True) #complete the  ...\n","\n","    ##Save the gradients in a dictionary to update the parameters\n","    grads = {\"dW_xh\": dW_xh,\n","             \"db_h\": db_h,\n","             \"dWo\": dWo,\n","             \"dbo\": dbo}\n","\n","    return grads\n","\n","#check!\n","##grads = back_prop(parameters, cache, train_scaled, one_hot(train_y))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGgdCd3XcmWp"},"source":["##Update \n","\n","Now we have saved all our gradient information from the backward pass. It is time to update our parameters. This means that we will chage their values in the direction that minimizes the cost.\n","\n","We have different dtrategies to update our parameters, but we will use the simple gradient descent algorithm. The parameter updates will be given by:\n","\n","$W := W - \\alpha . dW$\n","\n","$b := b - \\alpha . db$\n","\n","The intuition is that we update each value in our parameters 'a little' (we control how much through the learning rate lr = $\\alpha$) in the direction that minimizes the gradient."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hu7KDnVVclAk","executionInfo":{"status":"ok","timestamp":1637389472431,"user_tz":-540,"elapsed":389,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}},"outputId":"bef6b533-c57a-41e8-9517-d59cc4924dae"},"source":["def update(parameters, grads, lr):\n","    \n","    #Load your current parameters from the forward prop step\n","    W_xh = parameters[\"W_xh\"]\n","    b_h = parameters[\"b_h\"]\n","    Wo = parameters[\"Wo\"]\n","    bo = parameters[\"bo\"]\n","    \n","    #Load the derivatives of your parameters from the backward step\n","    dW_xh = grads[\"dW_xh\"]\n","    db_h = grads[\"db_h\"]\n","    dWo = grads[\"dWo\"]\n","    dbo = grads[\"dbo\"]\n","\n","    #Update your parameters using the gradient descent algorithm\n","    W_xh = W_xh - lr * dW_xh.T\n","    b_h = b_h - lr * db_h.T\n","    Wo = ... #complete the  ...\n","    bo = ... #complete the  ...\n","\n","    #Store your new parameters\n","    parameters = {\"W_xh\": W_xh,\n","                  \"b_h\": b_h,\n","                  \"Wo\": Wo,\n","                  \"bo\": bo}\n","\n","    return parameters\n","update(parameters, grads, lr)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'W_xh': array([[-1.55774025, -0.55499937, -0.16835097, ..., -0.2966716 ,\n","         -0.53903288, -0.69332885],\n","        [ 1.2696321 ,  1.50435164, -1.8259483 , ...,  0.08249186,\n","         -1.24478497, -1.1206462 ],\n","        [-2.19710091, -1.1843035 ,  0.09566839, ..., -0.53427223,\n","          0.45166789,  0.969216  ],\n","        ...,\n","        [-0.84453958, -1.11615351, -1.85565737, ..., -1.1208382 ,\n","          1.41601621, -1.29885875],\n","        [ 0.16231389, -1.37521115, -1.33226617, ...,  0.00365365,\n","          0.02775971,  0.12394979],\n","        [-0.39244818,  0.42103832, -0.16005907, ..., -0.19209251,\n","         -1.17253601,  1.07099013]]),\n"," 'Wo': array([[-0.11788564, -1.58551684, -0.54166225, ..., -0.20386236,\n","         -0.98964773, -1.31649244],\n","        [-1.34269869, -1.01850265,  0.56986165, ..., -2.59670563,\n","          0.8970028 , -0.36322349],\n","        [ 1.66114858,  2.15758466, -0.56842417, ...,  0.23124303,\n","         -0.67989288,  0.98769849],\n","        ...,\n","        [-0.1401697 , -0.18218567,  2.21040524, ...,  1.32787922,\n","          0.66512601,  0.81595116],\n","        [-1.03641476,  0.23636584, -1.6390993 , ...,  0.03851712,\n","         -0.73461619,  0.878816  ],\n","        [-0.28811521,  0.44415032,  0.03798192, ...,  0.45396811,\n","         -1.91015058, -0.76804577]]),\n"," 'b_h': array([[-1.09445916e-05,  8.26250671e-06,  1.22747964e-06,\n","         -9.25548321e-06,  8.15901384e-07, -3.63077410e-06,\n","         -1.22285232e-05,  1.01143282e-05,  1.15888926e-05,\n","         -3.86015007e-06,  5.42650772e-07,  1.15152577e-05,\n","         -2.96522151e-06, -1.23874820e-06,  1.68406927e-06,\n","         -1.82917854e-06, -3.57362863e-06,  4.42835962e-06,\n","         -4.71064964e-06, -3.03403140e-06, -1.75064278e-06,\n","          6.71907671e-07, -4.53806169e-06,  9.96011693e-07,\n","          1.33299139e-07, -2.80762170e-06,  1.55979933e-06,\n","          1.74701171e-07,  2.47358169e-06,  8.18043691e-07,\n","          9.53280658e-07,  3.24471910e-06,  1.60757518e-07,\n","          3.64795394e-06,  6.83640656e-06,  1.01138272e-05,\n","          7.53082408e-06, -1.04239153e-06, -3.61049144e-06,\n","         -1.44208201e-05, -3.64145829e-06,  1.36881133e-06,\n","         -6.06454639e-06, -1.27080720e-06,  1.98663969e-06,\n","          7.69384672e-06,  5.12508891e-06, -1.06915963e-05,\n","          1.91386428e-06,  3.18733026e-06, -5.69731360e-07,\n","          5.99185857e-07, -4.43581591e-07,  1.33292533e-06,\n","         -1.01524473e-05,  5.84420872e-06,  4.66244227e-06,\n","          3.23080018e-06,  3.57154002e-06, -4.45615896e-06,\n","         -1.14651588e-05, -6.62848396e-06,  1.97040879e-06,\n","          1.73569717e-05, -1.58384924e-06,  3.96202283e-06,\n","         -1.62826936e-06, -1.25139299e-06,  5.56151921e-06,\n","         -1.18583465e-06,  7.70233581e-06, -1.00023265e-06,\n","         -8.02729757e-06, -8.43654458e-07, -4.51062430e-07,\n","         -3.47006610e-06,  2.49756923e-07, -6.26972052e-08,\n","         -1.91186187e-05,  7.43796941e-07, -6.41166938e-06,\n","          1.33930440e-05, -2.16270090e-06,  1.67412662e-06,\n","          1.08107887e-05,  8.25132713e-06,  4.71348587e-06,\n","          2.97033723e-06,  1.14740590e-06, -1.84094582e-06,\n","         -5.15123922e-06, -1.06535486e-05, -7.88999019e-07,\n","         -4.32513928e-06,  9.91361421e-06,  3.59938485e-06,\n","          4.67951008e-06,  5.14828019e-06, -1.25211279e-06,\n","          2.70819669e-06,  7.61945754e-06, -3.21019976e-06,\n","          1.29878408e-06,  2.07863646e-06,  5.23991651e-06,\n","          1.27955100e-06, -4.24572730e-06,  2.81926869e-06,\n","         -4.52119089e-06, -2.43843932e-06, -2.78671058e-06,\n","          4.02612661e-06,  3.25778154e-06,  6.96856689e-08,\n","          3.88567342e-06, -1.62790002e-06, -3.44356188e-06,\n","          2.66336519e-06,  1.18106471e-06,  6.85370193e-06,\n","         -9.20877356e-06,  2.10461250e-06, -5.54697055e-06,\n","         -6.19552013e-06,  1.22462777e-05, -1.25161073e-06,\n","          2.35015331e-06, -8.88332994e-06,  2.12356829e-06,\n","          2.77036538e-06, -2.34662090e-06, -4.92177792e-06,\n","         -8.99953241e-06, -5.95814487e-06, -5.42909189e-06,\n","          9.34191848e-06,  8.60975552e-06,  4.33749004e-06,\n","          1.25266475e-05, -9.67720381e-07, -1.39954095e-05,\n","          8.40901985e-07,  1.05699091e-05,  4.31645075e-06,\n","          1.25819987e-05, -9.05885754e-06, -1.39353213e-06,\n","          4.74583628e-07,  7.00299658e-07,  9.62463479e-06,\n","          1.80254310e-06,  1.09802969e-06, -7.98258803e-06,\n","          4.01696778e-06, -3.01906975e-07,  9.78807740e-07,\n","         -7.61930123e-06, -2.59137001e-06, -4.98902923e-06,\n","         -1.96650627e-08, -1.20076472e-05,  1.25768733e-06,\n","          4.49980621e-06,  3.44680677e-06,  3.46404772e-06,\n","          4.61551253e-06,  2.82764888e-07,  9.98343398e-06,\n","         -2.61844631e-06,  5.88545632e-06, -7.63146718e-07,\n","         -1.66262610e-06,  1.20130347e-05,  1.01598465e-05,\n","          6.43027750e-06,  2.31169018e-06,  3.84361620e-06,\n","         -2.14262253e-06,  4.73798618e-07, -3.38207588e-06,\n","         -2.04876239e-06, -1.28319685e-06, -5.92234770e-06,\n","          1.52182031e-06,  7.15519314e-06, -1.51708521e-06,\n","          6.75234913e-06,  3.71106890e-06,  6.16639565e-07,\n","         -2.39467154e-06, -1.15817719e-05,  5.70183331e-07,\n","         -6.34162638e-06, -2.44001797e-06, -2.56734223e-06,\n","         -1.85886792e-06, -1.37663711e-06,  1.03336314e-05,\n","         -1.03973143e-05,  3.33727795e-06]]),\n"," 'bo': array([[-1.28921710e-04,  4.44178718e-05,  2.86331737e-04,\n","          1.58253501e-04,  4.38793699e-04,  1.01231211e-05,\n","          1.18978801e-04, -3.98780382e-04, -1.20859701e-04,\n","         -4.08336938e-04]])}"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"mw057LLil7PE"},"source":["##Almost done!\n","\n","Before putting everything together for training, let's define one more function. This will compute the accuracy of our predictions during training, and this way we can control how the model is working during training. It is not mandatory, but it is useful and later you can use it for testing."]},{"cell_type":"code","metadata":{"id":"phVn9J15mbRP"},"source":["def predict(prediction, labels):\n","    #get the label with the maximum probability in our estimation\n","    predictions = np.argmax(prediction, axis=0) \n","    #check our estimation wrt the true label\n","    new = np.array([labels[i] == predictions[i] for i,_ in enumerate(labels)], dtype=np.bool)\n","    #compute the accuracy\n","    acc = np.count_nonzero(new*1)/labels.shape[0]\n","    \n","    return acc * 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zc0UKJ-NcrmP"},"source":["## All together: the MLP training\n","\n","Congratulations, you survived! Let's put all together now :)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"O5pNF-4bcz7S","executionInfo":{"status":"error","timestamp":1637389516375,"user_tz":-540,"elapsed":26783,"user":{"displayName":"Daniela Rim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0Un9-xdDwQ6F_5rXjVKjKkX73nDVfporCxBLHlA=s64","userId":"03418673824746623906"}},"outputId":"c804b29c-ecf2-45b0-9d30-33561a38696d"},"source":["def MLP_model_train(X, train_y, H, O, D, lr, num_epochs, print_cost):\n","    \n","    ##initialize the parameters \n","    parameters = initp(D, H, O)\n","\n","    ##Load your initialized values\n","    W_xh = parameters[\"W_xh\"]\n","    b_h = parameters[\"b_h\"]\n","    Wo = parameters[\"Wo\"]\n","    bo = parameters[\"bo\"]\n","\n","    cost_per_epoch = []\n","    #Train!\n","    for i in range(0, num_epochs):\n","        #forward prop\n","        Ao, cache = forward_prop(...) #complete the  ...\n","\n","        #compute the cost and save it\n","        cost = nll_cost(..., one_hot(train_y), ...) #complete the  ...\n","        cost_per_epoch.append(cost)\n","\n","        #compute the ccuracy just for reference\n","        acc = predict(...) #complete the  ...\n","\n","        #backpropagate!\n","        grads = back_prop(...,...,..., one_hot(train_y)) #complete the  ...\n","\n","        #update!\n","        parameters = update(..., ..., ...) #complete the  ...\n","\n","        ##print the results every 10 epochs\n","        if print_cost and i % 10 == 0:\n","            print (\"Cost after iteration %i: %f with accuracy of %f\" %(i, cost, acc))\n","            \n","    print('Training ended.')\n","    return parameters, cost_per_epoch\n","\n","MLP_model_train(train_scaled.T, train_y, h, C, x_dim, lr, num_epochs, print_cost=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after iteration 0: 18.884961 with accuracy of 15.932000\n","Cost after iteration 10: 18.510577 with accuracy of 16.276000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-9f3e45a14d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mMLP_model_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-9f3e45a14d9d>\u001b[0m in \u001b[0;36mMLP_model_train\u001b[0;34m(X, train_y, H, O, D, lr, num_epochs, print_cost)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mAo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-64e5a5df808e>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"34XkNq8AudGy"},"source":["**How do we know if the training is running fine?**\n","\n","\n","1.   The cost should decrease in each epoch\n","2.   The accuracy should increase in each epoch\n","\n","**Does it take long to train and get a good accuracy?**\n","Yes, it does, so patience :)\n","\n","When do I stop training? When the cost does not change anymore and starts fluctuating around a certain value (it means it converged)"]},{"cell_type":"markdown","metadata":{"id":"50VWgpFHtOoq"},"source":["## Now it is your turn!\n","\n","What to do next: \n","\n","1. Check how much time it took you to train your num_epochs.\n","1.   Plot the training cost curve using the cost_per_epoch returned from your MLP_model_train() function.\n","2.   Define a function to test. I will give you the outline, but try to code it yourself. Report the final test accuracy.\n","\n","Done!\n","\n"]},{"cell_type":"code","metadata":{"id":"6SN_-kT6uSV3"},"source":["#This one is a pseudo-code, make sure it works :)\n","def test(parameters, test_im, test_labels):\n","\n","  ##Forward prop your new images using your trained parameters\n","  a_test, cache = forward_prop(test_im, parameters)\n","  ##Check the accuracy\n","  acc = predict(....)\n","  ##compute the cost function of the testing\n","  cost = nll_cost(....)\n","  return \"Cost for test dataset: %f with accuracy of %f\" %(cost, acc)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uNfIOplhwwR1"},"source":["**Can you do more?**\n","\n","Sure, have fun. If you want, you can try different number of neurons, learning rates, define a stopping criteria for the training (stop training if the cost converged), define a function to measure the time it takes to train, define a function to plot some examples of images and the labels that the model gives, etc."]}]}